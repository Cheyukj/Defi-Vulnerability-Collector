import re
import json
import requests
from bs4 import BeautifulSoup
import time
import random
import html

ua_list = [
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',
    'User-Agent:Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11',
    'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)',
    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',
    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1'
]

def get_keywords():
    return ["Security", "Threaten", "Defi", "Event"]

def crawler_blockthreat():
    url = 'https://substack.com/@blockthreat'
    headers = {'User-Agent': random.choice(ua_list)}

    try:
        response = requests.get(url=url, headers=headers)
        if response.status_code != 200:
            print(f"Error: Received status code {response.status_code} from {url}")
            return []
        print(f"Successfully fetched {url}")
    except Exception as e:
        print(f"Exception occurred while fetching {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    articles_to_save = []

    for item in soup.find_all('div', class_='post-preview'):
        try:
            title = item.find('div', class_='reader2-post-title reader2-clamp-lines').get_text().strip()
            content = item.find('p').get_text().strip()
            href = item.find('a')['href']
            timestamp = time.time()

            article_data = {
                'title': title,
                'description': content,
                'url': href,
                'source': 'BlockThreat',
                'timestamp': timestamp
            }
            articles_to_save.append(article_data)
            print(f"Found matching article: {title}\nContent: {content}")
        except AttributeError:
            print("Error parsing an item, skipping.")
            continue

    return articles_to_save

def crawler_fairyproof():
    url = 'https://substack.com/@fairyproof'
    headers = {'User-Agent': random.choice(ua_list)}

    try:
        response = requests.get(url=url, headers=headers)
        if response.status_code != 200:
            print(f"Error: Received status code {response.status_code} from {url}")
            return []
        print(f"Successfully fetched {url}")
    except Exception as e:
        print(f"Exception occurred while fetching {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    articles_to_save = []

    for item in soup.find_all('div', class_='post-preview'):
        try:
            title = item.find('div', class_='reader2-post-title reader2-clamp-lines').get_text().strip()
            content = item.find('p').get_text().strip()
            href = item.find('a')['href']
            timestamp = time.time()

            article_data = {
                'title': title,
                'description': content,
                'url': href,
                'source': 'FairyProof',
                'timestamp': timestamp
            }
            articles_to_save.append(article_data)
            print(f"Found matching article: {title}\nContent: {content}")
        except AttributeError:
            print("Error parsing an item, skipping.")
            continue

    return articles_to_save

def crawler_weekinethereumnews():
    url = 'https://substack.com/@weekinethereumnews'
    headers = {'User-Agent': random.choice(ua_list)}

    try:
        response = requests.get(url=url, headers=headers)
        if response.status_code != 200:
            print(f"Error: Received status code {response.status_code} from {url}")
            return []
        print(f"Successfully fetched {url}")
    except Exception as e:
        print(f"Exception occurred while fetching {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    articles_to_save = []

    for item in soup.find_all('div', class_='post-preview'):
        try:
            title = item.find('div', class_='pencraft pc-paddingTop-4 pc-reset _line-height-20_1k90y_95 _font-text_1k90y_121 _size-13_1k90y_45 _weight-regular_1k90y_159 _reset_1k90y_1').get_text().strip()
            content = item.find('p').get_text().strip()
            href = item.find('a')['href']
            timestamp = time.time()

            article_data = {
                'title': title,
                'description': content,
                'url': href,
                'source': 'WeekInEthereumNews',
                'timestamp': timestamp
            }
            articles_to_save.append(article_data)
            print(f"Found matching article: {title}\nContent: {content}")
        except AttributeError:
            print("Error parsing an item, skipping.")
            continue

    return articles_to_save

def crawler_todayindefi():
    url = 'https://substack.com/@todayindefi'
    headers = {'User-Agent': random.choice(ua_list)}

    try:
        response = requests.get(url=url, headers=headers)
        if response.status_code != 200:
            print(f"Error: Received status code {response.status_code} from {url}")
            return []
        print(f"Successfully fetched {url}")
    except Exception as e:
        print(f"Exception occurred while fetching {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    articles_to_save = []

    for item in soup.find_all('div', class_='post-preview'):
        try:
            title = item.find('div', class_='pencraft pc-display-flex pc-reset _linkRowA_214uo_50 reader2-inbox-post hover-action-menu _isClickable_214uo_57').get_text().strip()
            content = item.find('p').get_text().strip()
            href = item.find('a')['href']
            timestamp = time.time()

            article_data = {
                'title': title,
                'description': content,
                'url': href,
                'source': 'TodayInDeFi',
                'timestamp': timestamp
            }
            articles_to_save.append(article_data)
            print(f"Found matching article: {title}\nContent: {content}")
        except AttributeError:
            print("Error parsing an item, skipping.")
            continue

    return articles_to_save

if __name__ == '__main__':
    kw = get_keywords()
    pattern = '|'.join(kw)
    print('开始爬取新闻数据 ' + time.strftime("%H:%M:%S ", time.localtime()))

    news_set = crawler_blockthreat()
    for news in news_set:
        print('有一则 BlockThreat 文章:')
        for key, value in news.items():
            print(f"{key}: {value}")
        print('\n')

    news_set = crawler_fairyproof()
    for news in news_set:
        print('有一则 FairyProof 文章:')
        for key, value in news.items():
            print(f"{key}: {value}")
        print('\n')

    news_set = crawler_weekinethereumnews()
    for news in news_set:
        print('有一则 WeekInEthereumNews 文章:')
        for key, value in news.items():
            print(f"{key}: {value}")
        print('\n')

    news_set = crawler_todayindefi()
    for news in news_set:
        print('有一则 TodayInDeFi 文章:')
        for key, value in news.items():
            print(f"{key}: {value}")
        print('\n')
