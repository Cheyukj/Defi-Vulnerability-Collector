import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

def get_website_content(url):
    # 创建一个Session对象
    s = requests.Session()
    headers = {
        'User-Agent': UserAgent().random,  # 使用第三方库生成的随机UA
        'Referer': url,  # 添加防盗链，这个因网站而异
    }

    try:
        # 获取网站内容
        response = s.get(url, headers=headers)
        if response.status_code == 200:
            return response.text
        else:
            print(f"Failed to fetch content from {url}. Status code: {response.status_code}")
            return None
    except Exception as e:
        print(f"An error occurred while fetching content from {url}: {e}")
        return None

def extract_defi_info(html_content):
    try:
        # 使用BeautifulSoup解析HTML内容
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 提取目标信息
        text_content = soup.get_text(strip=True)  # 提取文本内容，去除多余空白字符
        
        return text_content
    except Exception as e:
        print(f"An error occurred while extracting defi info: {e}")
        return None

# 要爬取的网站列表
websites = [
    'https://blockthreat.substack.com/',
    'https://fairyproof.substack.com/',
    'https://todayindefi.substack.com/'
]

# 循环遍历爬取每个网站的内容
for website in websites:
    print(f"Fetching content from {website}:")
    content = get_website_content(website)
    if content:
        text_content = extract_defi_info(content)
        if text_content:
            print(text_content)
        print("-" * 100)
