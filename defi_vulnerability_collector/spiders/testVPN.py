import re
import json
import requests
from bs4 import BeautifulSoup
import time
import random

ua_list = [
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',
    'User-Agent:Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11',
    'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)',
    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0',
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1',
    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
]

def get_keywords():
    # 示例关键词列表
    return ["漏洞", "攻击", "安全", "黑客", "威胁"]

def crawler_chaincatcher(pattern):
    url = 'https://www.chaincatcher.com/api.php/content/lists?categoryid=3&page=1'
    headers = {'User-Agent': random.choice(ua_list)}
    req = requests.get(url=url, headers=headers)
    
    if req.status_code != 200:
        print(f"Error: Received status code {req.status_code} from {url}")
        return []

    try:
        data = req.json()
    except json.JSONDecodeError:
        print(f"Error: Unable to parse JSON from {url}")
        return []

    article_packs = data.get('data', [])
    articles_to_save = []
    for article_infos in article_packs:
        news_id = str(article_infos['contentid'])
        content = article_infos['description']
        if re.search(pattern, content, re.I):
            article = {
                'id': 'chaincatcher-' + news_id,
                'title': article_infos['title'],
                'description': article_infos['description'],
                'url': 'https://www.chaincatcher.com/article/' + news_id,
                'source': '链捕手',
                'timestamp': article_infos['addtime']
            }
            articles_to_save.append(article)
    return articles_to_save

def crawler_odaily(pattern):
    url = 'https://www.odaily.news/newsflash'
    headers = {'User-Agent': random.choice(ua_list)}
    html = requests.get(url=url, headers=headers)
    
    if html.status_code != 200:
        print(f"Error: Received status code {html.status_code} from {url}")
        return []

    soup = BeautifulSoup(html.text, 'lxml')
    script_tags = soup.select('script')
    
    if len(script_tags) < 2:
        print(f"Error: Not enough script tags found in {url}")
        return []

    data = script_tags[1].get_text()
    rawtext = data.split('\n')
    
    if len(rawtext) < 3:
        print(f"Error: Not enough lines found in script tag from {url}")
        return []

    rawtext_3 = rawtext[2][37:]
    try:
        json_text = json.loads(rawtext_3)
    except json.JSONDecodeError:
        print(f"Error: Unable to parse JSON from script tag in {url}")
        return []

    date_today = time.strftime("%Y-%m-%d", time.localtime())
    articles = json_text.get('newsflash', {}).get(date_today, [])
    articles_to_save = []
    for article in articles:
        news_id = str(article['id'])
        content = article['description']
        if re.search(pattern, content, re.I):
            article_data = {
                'id': 'odaily-' + news_id,
                'title': article['title'],
                'description': article['description'],
                'url': 'https://www.odaily.news/newsflash/' + news_id,
                'source': '星球日报',
                'timestamp': article['created_at']
            }
            articles_to_save.append(article_data)
    return articles_to_save

def crawler_PANews(pattern):
    url = 'https://www.panewslab.com/zh/news/index.html'
    headers = {'User-Agent':random.choice(ua_list)}
    html = requests.get(url=url,headers=headers)
    
    if html.status_code != 200:
        print(f"Error: Received status code {html.status_code} from {url}")
        return []

    soup = BeautifulSoup(html.text, 'lxml')
    articles_to_save = []
    for i in range(1, 4):
        path = f'div.kx-list:nth-child({i}) div.outer div.item'
        date_path = f'div.kx-list:nth-child({i}) div.outer div.kx-date'
        data = soup.select(path)
        date_data = soup.select(date_path)
        try:
            month = date_data[0].find(class_="month").get_text()[:-1]
            day = date_data[0].find(class_="date").get_text()
            if len(month) != 2:
                month = '0' + month
            if len(day) != 2:
                day = '0' + day
            year = time.strftime("%Y", time.localtime())
            date_of_news = int(month)
            local_date = int(time.strftime("%m", time.localtime()))
            if date_of_news > local_date:
                year = str((int(year) - 1))
            date = f"{year}-{month}-{day}"

            for items in data:
                news_id = items.find(class_="n-title").get('href')[-13:-5]
                title = items.find(class_="n-title").get_text()
                pubtime = items.find(class_="pubtime").get_text()
                main_text = items.find('p').get_text()
                date_time = f"{date} {pubtime}:00"
                timestamp = int(time.mktime(time.strptime(date_time, "%Y-%m-%d %H:%M:%S")))

                url = f'https://www.panewslab.com/zh/sqarticledetails/{news_id}.html'
                news_id = 'PANews-' + news_id
                source = 'PANews'

                if re.search(pattern, main_text, re.I):
                    article_data = {
                        'id': news_id,
                        'title': title,
                        'description': main_text,
                        'url': url,
                        'source': source,
                        'timestamp': timestamp
                    }
                    articles_to_save.append(article_data)
        except IndexError:
            pass
    return articles_to_save

def crawler_ForesightNews(pattern):
    url = 'http://foresightnews.pro/article'
    headers = {'User-Agent': random.choice(ua_list)}
    html = requests.get(url=url,headers=headers)
    
    if html.status_code != 200:
        print(f"Error: Received status code {html.status_code} from {url}")
        return []

    soup = BeautifulSoup(html.text, 'lxml')
    data = soup.select('#__layout > div > div:nth-child(1) > div.layout-body > div.app-content > div.z-container > div > div.z-col-md-42.z-col-xl-45 > div.box-padding > div:nth-child(1) > div')

    articles_to_save = []
    for items in data:
        href = items.find('a').get('href')
        title = items.find(class_="topic-body-title").get_text()
        main_text = items.find(class_='topic-body-content').get_text()
        news_id = re.findall(r'\d+', items.find('a').get('href'))[0]
        source = 'Foresight News'
        url = 'https://foresightnews.pro' + href
        news_id = 'ForesightNews-' + news_id
        try:
            date_time = items.find(class_='topic-time').get_text() + ':00'
        except AttributeError:
            date_time = items.find(class_='time').get_text() + ':00'
        timestamp = int(time.mktime(time.strptime(date_time, "%Y-%m-%d %H:%M:%S")))

        if re.search(pattern, main_text, re.I):
            article_data = {
                'id': news_id,
                'title': title,
                'description': main_text,
                'url': url,
                'source': source,
                'timestamp': timestamp
            }
            articles_to_save.append(article_data)
    return articles_to_save

if __name__ == '__main__':
    kw = get_keywords()
    pattern = '|'.join(kw)
    print('开始爬取新闻数据 ' + time.strftime("%H:%M:%S ", time.localtime()))

    news_set = crawler_chaincatcher(pattern)
    for news in news_set:
        print(news)

    news_set = crawler_odaily(pattern)
    for news in news_set:
        print(news)

    news_set = crawler_PANews(pattern)
    for news in news_set:
        print(news)

    news_set = crawler_ForesightNews(pattern)
    for news in news_set:
        print(news)
