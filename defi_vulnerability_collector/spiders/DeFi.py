import json
from bs4 import BeautifulSoup
import time
import random
import requests
from requests.adapters import HTTPAdapter
from dateutil.parser import parse
import re



# SSR配置代理
PROXIES = {'http': 'http://127.0.0.1:7890', 'https': 'http://127.0.0.1:7890'}

# 用户代理列表
ua_list = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0'
]

def get_response(url):
    headers = {'User-Agent': random.choice(ua_list)}
    session = requests.Session()
    session.mount('http://', HTTPAdapter(max_retries=3))
    session.mount('https://', HTTPAdapter(max_retries=3))
    response = session.get(url=url, headers=headers, timeout=10, proxies=PROXIES)
    response.raise_for_status()  # 如果请求失败则抛出异常
    return response

def crawler_substack(substack_url):
    try:
        response = get_response(substack_url)
    except Exception as e:
        print('crawler_substack:', e)
        return False

    # 打印网页内容，以便查看网页是否正确加载
    print(response.text)

    soup = BeautifulSoup(response.text, 'html.parser')

    # 打印 soup 变量，以便查看 BeautifulSoup 对象是否正确创建
    print(soup)

    data_list = []

    # 定位文章链接
    articles = soup.find_all('a', {'data-test': 'post-preview-title-link'})

    # 打印 articles 变量，以便查看是否成功找到文章
    print(articles)

    for article in articles:
        title = article.get_text().strip()
        url = article['href']
        abstract = article.find_next('p').get_text().strip()
        date_text = article.find_previous('time')['datetime']
        date = parse(date_text)
        time_stamp = int(date.timestamp())

        data_list.append({
            'title': title,
            'url': url,
            'abstract': abstract,
            'time_stamp': time_stamp
        })

    return data_list

if __name__ == '__main__':
    substack_urls = [
        'https://blockthreat.substack.com/',
        'https://fairyproof.substack.com/',
        'https://weekinethereumnews.substack.com/',
        'https://todayindefi.substack.com/'
    ]

    for url in substack_urls:
        articles = crawler_substack(url)
        if articles:
            print(f"Articles from {url}:")
            for article in articles:
                print("-" * 50)
                print("Title:", article['title'])
                print("URL:", article['url'])
                print("Abstract:", article['abstract'])
                print("Timestamp:", article['time_stamp'])
        else:
            print(f"Failed to fetch articles from {url}")
